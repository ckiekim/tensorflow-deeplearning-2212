{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 베스트 모델\n",
        "### - 이진 분류: 유방암 예측 사례"
      ],
      "metadata": {
        "id": "GuNkwiGsBqxt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QjXKSwppBjo0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터 준비"
      ],
      "metadata": {
        "id": "_PrCdNqnB4Jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "cancer = load_breast_cancer()"
      ],
      "metadata": {
        "id": "3DwTzXY0B248"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "cancer_std = scaler.fit_transform(cancer.data)"
      ],
      "metadata": {
        "id": "gXdj9xs4CIH8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    cancer_std, cancer.target, stratify=cancer.target, test_size=0.2, random_state=2022\n",
        ")"
      ],
      "metadata": {
        "id": "5H4Dsl3eCLZ7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 베스트 모델 저장하고 불러오기"
      ],
      "metadata": {
        "id": "TMnWN9L4CWp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 정의"
      ],
      "metadata": {
        "id": "YbmLS686FDfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "aiGCKjnNFCoN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Dense(100, input_dim=30, activation='relu'),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sopehhZSCPLD",
        "outputId": "6e8462b8-08f0-4b76-8e27-6144b924f319"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 100)               3100      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 24)                2424      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 25        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,549\n",
            "Trainable params: 5,549\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model compile\n",
        "model.compile('adam', 'binary_crossentropy', ['accuracy'])"
      ],
      "metadata": {
        "id": "fSczNmGeFLRL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Callback 설정 - 베스트 모델 저장"
      ],
      "metadata": {
        "id": "hWqjZLidFPqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model_path = 'best_model.h5'\n",
        "mc = ModelCheckpoint(\n",
        "    filepath=model_path,                # 저장할 파일 이름\n",
        "    monitor='val_loss',                 # validation dataset의 loss값 기준\n",
        "    verbose=1,                          # 화면에 잔뜩 표시\n",
        "    save_best_only=True                 # best model만 저장\n",
        ")"
      ],
      "metadata": {
        "id": "qgM6vp1gFgbl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 학습"
      ],
      "metadata": {
        "id": "KGFRGBPIGZ0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(X_train, y_train, validation_split=0.2, verbose=1,\n",
        "                 epochs=100, batch_size=50,\n",
        "                 callbacks=[mc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UFS0_MJGWzD",
        "outputId": "052ed7b4-dc63-4657-9813-8098dda19a86"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/8 [==>...........................] - ETA: 8s - loss: 0.6184 - accuracy: 0.7400\n",
            "Epoch 1: val_loss improved from inf to 0.40904, saving model to best_model.h5\n",
            "8/8 [==============================] - 2s 78ms/step - loss: 0.5098 - accuracy: 0.8379 - val_loss: 0.4090 - val_accuracy: 0.8791\n",
            "Epoch 2/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.3775 - accuracy: 0.9400\n",
            "Epoch 2: val_loss improved from 0.40904 to 0.30125, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.3335 - accuracy: 0.9148 - val_loss: 0.3013 - val_accuracy: 0.8901\n",
            "Epoch 3/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.2911 - accuracy: 0.9400\n",
            "Epoch 3: val_loss improved from 0.30125 to 0.24189, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2409 - accuracy: 0.9258 - val_loss: 0.2419 - val_accuracy: 0.9011\n",
            "Epoch 4/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.2555 - accuracy: 0.8800\n",
            "Epoch 4: val_loss improved from 0.24189 to 0.20691, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.1847 - accuracy: 0.9368 - val_loss: 0.2069 - val_accuracy: 0.9121\n",
            "Epoch 5/100\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.1493 - accuracy: 0.9457\n",
            "Epoch 5: val_loss improved from 0.20691 to 0.18274, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.1491 - accuracy: 0.9451 - val_loss: 0.1827 - val_accuracy: 0.9341\n",
            "Epoch 6/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0954 - accuracy: 0.9600\n",
            "Epoch 6: val_loss improved from 0.18274 to 0.16599, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.1245 - accuracy: 0.9643 - val_loss: 0.1660 - val_accuracy: 0.9451\n",
            "Epoch 7/100\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.1085 - accuracy: 0.9714\n",
            "Epoch 7: val_loss improved from 0.16599 to 0.15620, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1070 - accuracy: 0.9725 - val_loss: 0.1562 - val_accuracy: 0.9341\n",
            "Epoch 8/100\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.0878 - accuracy: 0.9800\n",
            "Epoch 8: val_loss improved from 0.15620 to 0.15040, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.0940 - accuracy: 0.9780 - val_loss: 0.1504 - val_accuracy: 0.9341\n",
            "Epoch 9/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1867 - accuracy: 0.9000\n",
            "Epoch 9: val_loss improved from 0.15040 to 0.14808, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0849 - accuracy: 0.9780 - val_loss: 0.1481 - val_accuracy: 0.9451\n",
            "Epoch 10/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0715 - accuracy: 0.9800\n",
            "Epoch 10: val_loss improved from 0.14808 to 0.14640, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0768 - accuracy: 0.9780 - val_loss: 0.1464 - val_accuracy: 0.9560\n",
            "Epoch 11/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0799 - accuracy: 0.9400\n",
            "Epoch 11: val_loss improved from 0.14640 to 0.14483, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0707 - accuracy: 0.9780 - val_loss: 0.1448 - val_accuracy: 0.9560\n",
            "Epoch 12/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0867 - accuracy: 0.9600\n",
            "Epoch 12: val_loss improved from 0.14483 to 0.14345, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0657 - accuracy: 0.9808 - val_loss: 0.1435 - val_accuracy: 0.9560\n",
            "Epoch 13/100\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.0631 - accuracy: 0.9800\n",
            "Epoch 13: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0613 - accuracy: 0.9808 - val_loss: 0.1441 - val_accuracy: 0.9560\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9808\n",
            "Epoch 14: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.0574 - accuracy: 0.9808 - val_loss: 0.1456 - val_accuracy: 0.9560\n",
            "Epoch 15/100\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.0560 - accuracy: 0.9800\n",
            "Epoch 15: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0540 - accuracy: 0.9808 - val_loss: 0.1466 - val_accuracy: 0.9560\n",
            "Epoch 16/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0340 - accuracy: 0.9800\n",
            "Epoch 16: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0509 - accuracy: 0.9808 - val_loss: 0.1471 - val_accuracy: 0.9560\n",
            "Epoch 17/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0540 - accuracy: 0.9600\n",
            "Epoch 17: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0476 - accuracy: 0.9835 - val_loss: 0.1479 - val_accuracy: 0.9560\n",
            "Epoch 18/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0389 - accuracy: 0.9800\n",
            "Epoch 18: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0454 - accuracy: 0.9835 - val_loss: 0.1502 - val_accuracy: 0.9451\n",
            "Epoch 19/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0666 - accuracy: 0.9600\n",
            "Epoch 19: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0431 - accuracy: 0.9835 - val_loss: 0.1530 - val_accuracy: 0.9451\n",
            "Epoch 20/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0130 - accuracy: 1.0000\n",
            "Epoch 20: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.0409 - accuracy: 0.9835 - val_loss: 0.1545 - val_accuracy: 0.9451\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9835\n",
            "Epoch 21: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0401 - accuracy: 0.9835 - val_loss: 0.1552 - val_accuracy: 0.9560\n",
            "Epoch 22/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0284 - accuracy: 1.0000\n",
            "Epoch 22: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0370 - accuracy: 0.9863 - val_loss: 0.1588 - val_accuracy: 0.9451\n",
            "Epoch 23/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0485 - accuracy: 0.9800\n",
            "Epoch 23: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0345 - accuracy: 0.9863 - val_loss: 0.1619 - val_accuracy: 0.9451\n",
            "Epoch 24/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0289 - accuracy: 1.0000\n",
            "Epoch 24: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0332 - accuracy: 0.9863 - val_loss: 0.1641 - val_accuracy: 0.9451\n",
            "Epoch 25/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0644 - accuracy: 0.9600\n",
            "Epoch 25: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0313 - accuracy: 0.9863 - val_loss: 0.1665 - val_accuracy: 0.9451\n",
            "Epoch 26/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0138 - accuracy: 1.0000\n",
            "Epoch 26: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0297 - accuracy: 0.9863 - val_loss: 0.1751 - val_accuracy: 0.9451\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9890\n",
            "Epoch 27: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.0284 - accuracy: 0.9890 - val_loss: 0.1766 - val_accuracy: 0.9451\n",
            "Epoch 28/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0113 - accuracy: 1.0000\n",
            "Epoch 28: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0269 - accuracy: 0.9890 - val_loss: 0.1762 - val_accuracy: 0.9451\n",
            "Epoch 29/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0328 - accuracy: 1.0000\n",
            "Epoch 29: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0251 - accuracy: 0.9945 - val_loss: 0.1755 - val_accuracy: 0.9451\n",
            "Epoch 30/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0343 - accuracy: 1.0000\n",
            "Epoch 30: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0238 - accuracy: 0.9945 - val_loss: 0.1778 - val_accuracy: 0.9451\n",
            "Epoch 31/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0348 - accuracy: 0.9800\n",
            "Epoch 31: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0224 - accuracy: 0.9945 - val_loss: 0.1805 - val_accuracy: 0.9451\n",
            "Epoch 32/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0127 - accuracy: 1.0000\n",
            "Epoch 32: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0212 - accuracy: 0.9945 - val_loss: 0.1817 - val_accuracy: 0.9451\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9945\n",
            "Epoch 33: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.0203 - accuracy: 0.9945 - val_loss: 0.1838 - val_accuracy: 0.9451\n",
            "Epoch 34/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 34: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0195 - accuracy: 0.9945 - val_loss: 0.1881 - val_accuracy: 0.9451\n",
            "Epoch 35/100\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.0187 - accuracy: 0.9971\n",
            "Epoch 35: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.0183 - accuracy: 0.9973 - val_loss: 0.1924 - val_accuracy: 0.9451\n",
            "Epoch 36/100\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.0159 - accuracy: 0.9967\n",
            "Epoch 36: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.0173 - accuracy: 0.9973 - val_loss: 0.1952 - val_accuracy: 0.9451\n",
            "Epoch 37/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0266 - accuracy: 1.0000\n",
            "Epoch 37: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.1975 - val_accuracy: 0.9451\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 1.0000\n",
            "Epoch 38: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.2015 - val_accuracy: 0.9451\n",
            "Epoch 39/100\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.0157 - accuracy: 1.0000\n",
            "Epoch 39: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.2015 - val_accuracy: 0.9451\n",
            "Epoch 40/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0102 - accuracy: 1.0000\n",
            "Epoch 40: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.2014 - val_accuracy: 0.9451\n",
            "Epoch 41/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0296 - accuracy: 1.0000\n",
            "Epoch 41: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.2055 - val_accuracy: 0.9451\n",
            "Epoch 42/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0101 - accuracy: 1.0000\n",
            "Epoch 42: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.2089 - val_accuracy: 0.9451\n",
            "Epoch 43/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 43: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.2193 - val_accuracy: 0.9451\n",
            "Epoch 44/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 44: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.2204 - val_accuracy: 0.9451\n",
            "Epoch 45/100\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.0110 - accuracy: 1.0000\n",
            "Epoch 45: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.2166 - val_accuracy: 0.9451\n",
            "Epoch 46/100\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 46: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.2150 - val_accuracy: 0.9451\n",
            "Epoch 47/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 47: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.2156 - val_accuracy: 0.9451\n",
            "Epoch 48/100\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.0101 - accuracy: 1.0000\n",
            "Epoch 48: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.2163 - val_accuracy: 0.9451\n",
            "Epoch 49/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 49: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.2186 - val_accuracy: 0.9451\n",
            "Epoch 50/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0096 - accuracy: 1.0000\n",
            "Epoch 50: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9451\n",
            "Epoch 51/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0158 - accuracy: 1.0000\n",
            "Epoch 51: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2261 - val_accuracy: 0.9451\n",
            "Epoch 52/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 52: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.2256 - val_accuracy: 0.9451\n",
            "Epoch 53/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 53: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.2276 - val_accuracy: 0.9451\n",
            "Epoch 54/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0127 - accuracy: 1.0000\n",
            "Epoch 54: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.2288 - val_accuracy: 0.9451\n",
            "Epoch 55/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0125 - accuracy: 1.0000\n",
            "Epoch 55: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.2351 - val_accuracy: 0.9451\n",
            "Epoch 56/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 56: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.2365 - val_accuracy: 0.9451\n",
            "Epoch 57/100\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 57: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.2361 - val_accuracy: 0.9451\n",
            "Epoch 58/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 58: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.2359 - val_accuracy: 0.9451\n",
            "Epoch 59/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 59: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.2361 - val_accuracy: 0.9451\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 60: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.2403 - val_accuracy: 0.9451\n",
            "Epoch 61/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 61: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.2420 - val_accuracy: 0.9451\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 62: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.2420 - val_accuracy: 0.9451\n",
            "Epoch 63/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 63: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.2429 - val_accuracy: 0.9451\n",
            "Epoch 64/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 64: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.2443 - val_accuracy: 0.9451\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 65: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.2481 - val_accuracy: 0.9451\n",
            "Epoch 66/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 66: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.2509 - val_accuracy: 0.9451\n",
            "Epoch 67/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 67: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2479 - val_accuracy: 0.9451\n",
            "Epoch 68/100\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000    \n",
            "Epoch 68: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.2482 - val_accuracy: 0.9451\n",
            "Epoch 69/100\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 69: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2474 - val_accuracy: 0.9451\n",
            "Epoch 70/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 70: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2467 - val_accuracy: 0.9451\n",
            "Epoch 71/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 71: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2441 - val_accuracy: 0.9451\n",
            "Epoch 72/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 72: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2451 - val_accuracy: 0.9451\n",
            "Epoch 73/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 73: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.2451 - val_accuracy: 0.9451\n",
            "Epoch 74/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 74: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.2487 - val_accuracy: 0.9451\n",
            "Epoch 75/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 75: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.2508 - val_accuracy: 0.9451\n",
            "Epoch 76/100\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 76: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2499 - val_accuracy: 0.9451\n",
            "Epoch 77/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 77: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2513 - val_accuracy: 0.9451\n",
            "Epoch 78/100\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 78: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2549 - val_accuracy: 0.9451\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 79: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2555 - val_accuracy: 0.9451\n",
            "Epoch 80/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 5.4448e-04 - accuracy: 1.0000\n",
            "Epoch 80: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.2569 - val_accuracy: 0.9451\n",
            "Epoch 81/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 7.0823e-04 - accuracy: 1.0000\n",
            "Epoch 81: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.2642 - val_accuracy: 0.9451\n",
            "Epoch 82/100\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 82: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2665 - val_accuracy: 0.9451\n",
            "Epoch 83/100\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000    \n",
            "Epoch 83: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.2663 - val_accuracy: 0.9451\n",
            "Epoch 84/100\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 84: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2675 - val_accuracy: 0.9451\n",
            "Epoch 85/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 85: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2688 - val_accuracy: 0.9451\n",
            "Epoch 86/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 7.7689e-04 - accuracy: 1.0000\n",
            "Epoch 86: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.2686 - val_accuracy: 0.9451\n",
            "Epoch 87/100\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000    \n",
            "Epoch 87: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2678 - val_accuracy: 0.9451\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 88: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2684 - val_accuracy: 0.9451\n",
            "Epoch 89/100\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
            "Epoch 89: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2699 - val_accuracy: 0.9451\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000    \n",
            "Epoch 90: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2712 - val_accuracy: 0.9451\n",
            "Epoch 91/100\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 91: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2725 - val_accuracy: 0.9451\n",
            "Epoch 92/100\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 92: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2740 - val_accuracy: 0.9451\n",
            "Epoch 93/100\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    \n",
            "Epoch 93: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2737 - val_accuracy: 0.9451\n",
            "Epoch 94/100\n",
            "5/8 [=================>............] - ETA: 0s - loss: 9.8658e-04 - accuracy: 1.0000\n",
            "Epoch 94: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2729 - val_accuracy: 0.9451\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
            "Epoch 95: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2772 - val_accuracy: 0.9451\n",
            "Epoch 96/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 96: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2789 - val_accuracy: 0.9451\n",
            "Epoch 97/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 8.1874e-04 - accuracy: 1.0000\n",
            "Epoch 97: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2802 - val_accuracy: 0.9451\n",
            "Epoch 98/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 5.1045e-04 - accuracy: 1.0000\n",
            "Epoch 98: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2802 - val_accuracy: 0.9451\n",
            "Epoch 99/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 7.0317e-04 - accuracy: 1.0000\n",
            "Epoch 99: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2800 - val_accuracy: 0.9451\n",
            "Epoch 100/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 4.9219e-04 - accuracy: 1.0000\n",
            "Epoch 100: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2803 - val_accuracy: 0.9451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 베스트 모델 불러오기"
      ],
      "metadata": {
        "id": "dC1sbs8HHkl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "best_model = load_model(model_path)"
      ],
      "metadata": {
        "id": "K_GSJgaYG3dt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 베스트 모델로 평가"
      ],
      "metadata": {
        "id": "jPmn9SyYH180"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUZS5f65Hzy6",
        "outputId": "8ca96ebe-7772-4c74-c5db-504fde20631a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0346 - accuracy: 0.9912\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.034577008336782455, 0.9912280440330505]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 훈련 시각화"
      ],
      "metadata": {
        "id": "SOUuaXOkIn2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_acc = hist.history['accuracy']\n",
        "y_vloss = hist.history['val_loss']\n",
        "xs = np.arange(1, len(y_acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(xs, y_acc, label='train accuracy')\n",
        "plt.plot(xs, y_vloss, label='validation loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.ylim([0,1.1])\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "jrtXtRGmIm4E",
        "outputId": "dd31804a-33ef-46d8-ec63-81b2facb8a77"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 베스트 모델 저장하고 조기 종료하기"
      ],
      "metadata": {
        "id": "zopNg5LNId5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dropout layer 추가"
      ],
      "metadata": {
        "id": "wvZ7C675JSJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "model2 = Sequential([\n",
        "    Dense(100, input_dim=30, activation='relu'),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZDmUI2zIBNR",
        "outputId": "9d0edb05-a6d9-43da-c571-59cee62da545"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 100)               3100      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 24)                2424      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 24)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 25        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,549\n",
            "Trainable params: 5,549\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model compile\n",
        "model2.compile('adam', 'binary_crossentropy', ['accuracy'])"
      ],
      "metadata": {
        "id": "gF6b8v6_J5ay"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 조기 종료 조건 설정"
      ],
      "metadata": {
        "id": "FqsAESTWKOYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=20                     # 20 epoch 동안 val_loss가 좋아지지 않으면 강제 종료\n",
        ")"
      ],
      "metadata": {
        "id": "BHTPl36FKMOB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path2 = 'best_model2.h5'\n",
        "mc2 = ModelCheckpoint(filepath=model_path2, monitor='val_loss', verbose=1,\n",
        "                      save_best_only=True)"
      ],
      "metadata": {
        "id": "Dk4O1_ppLPyK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist2 = model2.fit(X_train, y_train, validation_split=0.2,\n",
        "                   verbose=1, epochs=100, batch_size=50,\n",
        "                   callbacks=[mc, es])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK_7XDz1Kwya",
        "outputId": "6c3cf17e-f9a7-4ad2-ac28-04f592e9ae36"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/8 [==>...........................] - ETA: 3s - loss: 0.6625 - accuracy: 0.6600\n",
            "Epoch 1: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 1s 29ms/step - loss: 0.6031 - accuracy: 0.6676 - val_loss: 0.4573 - val_accuracy: 0.8901\n",
            "Epoch 2/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.5179 - accuracy: 0.7800\n",
            "Epoch 2: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4716 - accuracy: 0.8269 - val_loss: 0.3414 - val_accuracy: 0.9231\n",
            "Epoch 3/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.3655 - accuracy: 0.9000\n",
            "Epoch 3: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3501 - accuracy: 0.9066 - val_loss: 0.2686 - val_accuracy: 0.9231\n",
            "Epoch 4/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.3271 - accuracy: 0.8400\n",
            "Epoch 4: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2749 - accuracy: 0.9176 - val_loss: 0.2198 - val_accuracy: 0.9341\n",
            "Epoch 5/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.2029 - accuracy: 0.9200\n",
            "Epoch 5: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2340 - accuracy: 0.9286 - val_loss: 0.1869 - val_accuracy: 0.9451\n",
            "Epoch 6/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.2302 - accuracy: 0.9000\n",
            "Epoch 6: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1885 - accuracy: 0.9478 - val_loss: 0.1636 - val_accuracy: 0.9451\n",
            "Epoch 7/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1444 - accuracy: 0.9800\n",
            "Epoch 7: val_loss did not improve from 0.14345\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1752 - accuracy: 0.9478 - val_loss: 0.1475 - val_accuracy: 0.9560\n",
            "Epoch 8/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1609 - accuracy: 0.9400\n",
            "Epoch 8: val_loss improved from 0.14345 to 0.13748, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.1506 - accuracy: 0.9560 - val_loss: 0.1375 - val_accuracy: 0.9560\n",
            "Epoch 9/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.2577 - accuracy: 0.9000\n",
            "Epoch 9: val_loss improved from 0.13748 to 0.12979, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1461 - accuracy: 0.9478 - val_loss: 0.1298 - val_accuracy: 0.9670\n",
            "Epoch 10/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1003 - accuracy: 0.9800\n",
            "Epoch 10: val_loss improved from 0.12979 to 0.12390, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1133 - accuracy: 0.9725 - val_loss: 0.1239 - val_accuracy: 0.9560\n",
            "Epoch 11/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1099 - accuracy: 0.9600\n",
            "Epoch 11: val_loss improved from 0.12390 to 0.11915, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1236 - accuracy: 0.9643 - val_loss: 0.1192 - val_accuracy: 0.9560\n",
            "Epoch 12/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0664 - accuracy: 1.0000\n",
            "Epoch 12: val_loss improved from 0.11915 to 0.11601, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.1058 - accuracy: 0.9698 - val_loss: 0.1160 - val_accuracy: 0.9560\n",
            "Epoch 13/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1191 - accuracy: 0.9600\n",
            "Epoch 13: val_loss improved from 0.11601 to 0.11364, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0989 - accuracy: 0.9780 - val_loss: 0.1136 - val_accuracy: 0.9560\n",
            "Epoch 14/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1672 - accuracy: 0.9600\n",
            "Epoch 14: val_loss improved from 0.11364 to 0.11212, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0903 - accuracy: 0.9725 - val_loss: 0.1121 - val_accuracy: 0.9560\n",
            "Epoch 15/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0543 - accuracy: 1.0000\n",
            "Epoch 15: val_loss improved from 0.11212 to 0.11195, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0804 - accuracy: 0.9808 - val_loss: 0.1120 - val_accuracy: 0.9451\n",
            "Epoch 16/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0516 - accuracy: 0.9800\n",
            "Epoch 16: val_loss did not improve from 0.11195\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0815 - accuracy: 0.9753 - val_loss: 0.1127 - val_accuracy: 0.9451\n",
            "Epoch 17/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0573 - accuracy: 0.9800\n",
            "Epoch 17: val_loss did not improve from 0.11195\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0713 - accuracy: 0.9780 - val_loss: 0.1120 - val_accuracy: 0.9451\n",
            "Epoch 18/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1355 - accuracy: 0.9200\n",
            "Epoch 18: val_loss improved from 0.11195 to 0.11078, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0710 - accuracy: 0.9725 - val_loss: 0.1108 - val_accuracy: 0.9451\n",
            "Epoch 19/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0197 - accuracy: 1.0000\n",
            "Epoch 19: val_loss improved from 0.11078 to 0.11011, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0687 - accuracy: 0.9780 - val_loss: 0.1101 - val_accuracy: 0.9451\n",
            "Epoch 20/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0936 - accuracy: 0.9800\n",
            "Epoch 20: val_loss did not improve from 0.11011\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0694 - accuracy: 0.9835 - val_loss: 0.1102 - val_accuracy: 0.9451\n",
            "Epoch 21/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0380 - accuracy: 1.0000\n",
            "Epoch 21: val_loss improved from 0.11011 to 0.10985, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0659 - accuracy: 0.9863 - val_loss: 0.1098 - val_accuracy: 0.9451\n",
            "Epoch 22/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0271 - accuracy: 1.0000\n",
            "Epoch 22: val_loss improved from 0.10985 to 0.10944, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0655 - accuracy: 0.9753 - val_loss: 0.1094 - val_accuracy: 0.9451\n",
            "Epoch 23/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1061 - accuracy: 0.9800\n",
            "Epoch 23: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0579 - accuracy: 0.9835 - val_loss: 0.1101 - val_accuracy: 0.9560\n",
            "Epoch 24/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0332 - accuracy: 1.0000\n",
            "Epoch 24: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0504 - accuracy: 0.9863 - val_loss: 0.1140 - val_accuracy: 0.9560\n",
            "Epoch 25/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0480 - accuracy: 1.0000\n",
            "Epoch 25: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0586 - accuracy: 0.9835 - val_loss: 0.1155 - val_accuracy: 0.9560\n",
            "Epoch 26/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0498 - accuracy: 0.9600\n",
            "Epoch 26: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0588 - accuracy: 0.9835 - val_loss: 0.1152 - val_accuracy: 0.9560\n",
            "Epoch 27/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0358 - accuracy: 0.9800\n",
            "Epoch 27: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0486 - accuracy: 0.9890 - val_loss: 0.1154 - val_accuracy: 0.9560\n",
            "Epoch 28/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1086 - accuracy: 0.9600\n",
            "Epoch 28: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0408 - accuracy: 0.9890 - val_loss: 0.1158 - val_accuracy: 0.9560\n",
            "Epoch 29/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0113 - accuracy: 1.0000\n",
            "Epoch 29: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0442 - accuracy: 0.9835 - val_loss: 0.1179 - val_accuracy: 0.9560\n",
            "Epoch 30/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0150 - accuracy: 1.0000\n",
            "Epoch 30: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0421 - accuracy: 0.9918 - val_loss: 0.1184 - val_accuracy: 0.9560\n",
            "Epoch 31/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0502 - accuracy: 0.9800\n",
            "Epoch 31: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0435 - accuracy: 0.9918 - val_loss: 0.1198 - val_accuracy: 0.9560\n",
            "Epoch 32/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0216 - accuracy: 1.0000\n",
            "Epoch 32: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0428 - accuracy: 0.9918 - val_loss: 0.1256 - val_accuracy: 0.9560\n",
            "Epoch 33/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0211 - accuracy: 1.0000\n",
            "Epoch 33: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0397 - accuracy: 0.9863 - val_loss: 0.1285 - val_accuracy: 0.9560\n",
            "Epoch 34/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 34: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0403 - accuracy: 0.9863 - val_loss: 0.1298 - val_accuracy: 0.9560\n",
            "Epoch 35/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
            "Epoch 35: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0380 - accuracy: 0.9945 - val_loss: 0.1306 - val_accuracy: 0.9560\n",
            "Epoch 36/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0360 - accuracy: 1.0000\n",
            "Epoch 36: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0359 - accuracy: 0.9890 - val_loss: 0.1338 - val_accuracy: 0.9560\n",
            "Epoch 37/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0142 - accuracy: 1.0000\n",
            "Epoch 37: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0308 - accuracy: 0.9945 - val_loss: 0.1369 - val_accuracy: 0.9670\n",
            "Epoch 38/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0597 - accuracy: 0.9600\n",
            "Epoch 38: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0361 - accuracy: 0.9890 - val_loss: 0.1392 - val_accuracy: 0.9670\n",
            "Epoch 39/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 39: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0288 - accuracy: 0.9918 - val_loss: 0.1419 - val_accuracy: 0.9670\n",
            "Epoch 40/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0321 - accuracy: 1.0000\n",
            "Epoch 40: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0284 - accuracy: 0.9973 - val_loss: 0.1427 - val_accuracy: 0.9670\n",
            "Epoch 41/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0542 - accuracy: 0.9800\n",
            "Epoch 41: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0264 - accuracy: 0.9945 - val_loss: 0.1415 - val_accuracy: 0.9670\n",
            "Epoch 42/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0111 - accuracy: 1.0000\n",
            "Epoch 42: val_loss did not improve from 0.10944\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0264 - accuracy: 0.9973 - val_loss: 0.1434 - val_accuracy: 0.9560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model2 = load_model(model_path2)\n",
        "best_model2.evaluate(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYFiuxQPLpQq",
        "outputId": "b0116335-5888-41de-a280-d310de67662a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0574 - accuracy: 0.9802\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05743720009922981, 0.9802197813987732]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e5Tz8vOYMwRp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}